{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import ngrams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import everygrams\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"speeches.txt\", \"r\")\n",
    "donald = f.read()\n",
    "donald=donald.lower()\n",
    "sent_tokenize_list = sent_tokenize(donald)\n",
    "for i in range(len(sent_tokenize_list)):\n",
    "    sent_tokenize_list[i]=re.sub(r'[^\\w]', ' ',sent_tokenize_list[i])\n",
    "X_train, X_test = train_test_split(sent_tokenize_list, test_size=0.2 , random_state=42)\n",
    "vocab=[]\n",
    "token=[]\n",
    "for i in range(len(X_train)):\n",
    "    temp=word_tokenize(X_train[i])\n",
    "    token.append('<s>')\n",
    "    token.extend(temp)\n",
    "    token.append('</s>')\n",
    "vocab=np.unique(token)\n",
    "# print(vocab[3000])\n",
    "output_sent=[]\n",
    "token2=[]\n",
    "for i in range(len(X_test)):\n",
    "    temp=word_tokenize(X_test[i])\n",
    "    token1=[]\n",
    "    token1.append('<s>')\n",
    "    token1.extend(temp)\n",
    "    token1.append('</s>')\n",
    "    output_sent.append(token1)\n",
    "    token2.extend(token1)\n",
    "total_vocab=np.unique(np.concatenate((token2,vocab),axis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of  1 -gram Theoretically possible:  5406\n",
      "No of  1 -gram Observed:  5406\n",
      "No of  2 -gram Theoretically possible:  29224836\n",
      "No of  2 -gram Observed:  41930\n",
      "No of  3 -gram Theoretically possible:  157989463416\n",
      "No of  3 -gram Observed:  84828\n",
      "No of  4 -gram Theoretically possible:  854091039226896\n",
      "No of  4 -gram Observed:  118036\n"
     ]
    }
   ],
   "source": [
    "def ngram(n):\n",
    "    input_ngram = list(everygrams(token,max_len=n))\n",
    "    lm = MLE(n)\n",
    "    lm.fit([input_ngram], vocabulary_text=vocab)\n",
    "    return lm\n",
    "for i in range(1,5):\n",
    "    print(\"No of \",i,\"- gram Theoretically possible: \",len(vocab)**i)\n",
    "    print(\"No of \",i,\"- gram Observed: \",len(set(ngrams(token,i))))\n",
    "uni=ngram(1)\n",
    "bi=ngram(2)\n",
    "tri=ngram(3)\n",
    "quad=ngram(4)\n",
    "# print(\"lo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram:\n",
      "<s> why and be s t </s>\n",
      "<s> </s>\n",
      "<s> t but evangelical </s>\n",
      "<s> believe magazine m </s>\n",
      "<s> <s> what thousands re have it be all john every is </s>\n",
      "<s> </s>\n",
      "<s> <s> didn able last get the to m a being and on helpless going these have wall why probably enjoy is t going they </s>\n",
      "<s> and our trying way would states </s>\n",
      "<s> have in anything important <s> rebuild is border </s>\n",
      "<s> is watch the right has i right hell terrible think executive the </s>\n",
      "\n",
      "Bigram:\n",
      "<s> i don t masterminds and i think that fought for all of </s>\n",
      "<s> it back in the controversial </s>\n",
      "<s> the apprentice continues to go home with the thing </s>\n",
      "<s> we have to know when i don t have a certain way this is not as president obama announces because their puppet states permanently admits more than military interventions will not want to make good prognosticator i killed </s>\n",
      "<s> she did you finish prisoners that he did it great it s pretty close </s>\n",
      "<s> i made the arm necessarily </s>\n",
      "<s> i don t win anything else by iran deal </s>\n",
      "<s> you had 24 hours </s>\n",
      "<s> they flying these people that hillary </s>\n",
      "<s> six months i beat isis to just took i don t know i have the vets </s>\n",
      "\n",
      "Trigram:\n",
      "<s> and when allies abandoned it to of do respect the trade the level nine <s> this every it </s>\n",
      "<s> that s the <s> to lot york that and literally </s>\n",
      "<s> it doesn was on des i we get they because the and into go just <s> </s>\n",
      "<s> they write don we trade bottom going i been to we </s>\n",
      "<s> she s wants </s>\n",
      "<s> we re re their i get and have be tremendous problem make they <s> he whether the but right my going ever with out that how we i know mean to <s> cycles have saying </s>\n",
      "<s> is anybody </s>\n",
      "<s> it s s </s>\n",
      "<s> i ve decided </s>\n",
      "<s> i m got m we obviously good and thing and they people </s>\n",
      "\n",
      "Quadgram:\n",
      "<s> so it really s s just really </s>\n",
      "<s> i mean four i for my the <s> in show thought a </s>\n",
      "<s> i ll give tell order our to </s>\n",
      "<s> and i don built never m we we have somebody 20s think </s>\n",
      "<s> it was approved genesco such hatred address <s> behind something right won company women born six at weakening guns <s> have vote we fighting re fraud astoria </s>\n",
      "<s> you sign your your your all veterans you take to and very <s> our i work i what it had that </s>\n",
      "<s> but he can used said shouldn be trucks if <s> of and and provided do i and <s> </s>\n",
      "<s> you know this they why our it say </s>\n",
      "<s> now our president country country was if to many are wall re they t people they muslim a about we they </s>\n",
      "<s> 20 000 people people miles hands you t it they pack would people i s <s> inversion or <s> seen will people out negotiating know i and <s> or emergency was we tell come 000 backed here generous action israel win but and to </s>\n"
     ]
    }
   ],
   "source": [
    "def generate(n,model):\n",
    "    start=\"<s>\"\n",
    "    sentence=[start]\n",
    "    if(n==1):\n",
    "        while(start!=\"</s>\"):\n",
    "            index=np.argmax(np.random.multinomial(1,[model.score(i) for i in vocab],size = None))\n",
    "            sentence.append(vocab[index])\n",
    "            start=vocab[index]\n",
    "    elif(n==2):\n",
    "        while(start!=\"</s>\"):\n",
    "            index=np.argmax(np.random.multinomial(1,[model.score(i,[start]) for i in vocab],size = None))\n",
    "            sentence.append(vocab[index])\n",
    "            start=vocab[index]\n",
    "    elif(n==3):\n",
    "        start_arr=[start]\n",
    "        index=np.argmax(np.random.multinomial(1,[model.score(i,[start]) for i in vocab],size = None))\n",
    "        sentence.append(vocab[index])\n",
    "        start_arr.append(vocab[index])\n",
    "        while(start!=\"</s>\"):\n",
    "            index=np.argmax(np.random.multinomial(1,[model.score(i,start_arr) for i in vocab],size = None))\n",
    "            sentence.append(vocab[index])\n",
    "            start=vocab[index]\n",
    "            start_arr=start_arr[1:]\n",
    "    elif(n==4):\n",
    "        start_arr=[start]\n",
    "        index=np.argmax(np.random.multinomial(1,[model.score(i,[start]) for i in vocab],size = None))\n",
    "        sentence.append(vocab[index])\n",
    "        start_arr.append(vocab[index])\n",
    "        index=np.argmax(np.random.multinomial(1,[model.score(i,start_arr) for i in vocab],size = None))\n",
    "        sentence.append(vocab[index])\n",
    "        start_arr.append(vocab[index])\n",
    "        while(start!=\"</s>\"):\n",
    "            index=np.argmax(np.random.multinomial(1,[model.score(i,start_arr) for i in vocab],size = None))\n",
    "            sentence.append(vocab[index])\n",
    "            start=vocab[index]\n",
    "            start_arr=start_arr[1:]\n",
    "    return ' '.join(sentence)\n",
    "print(\"Unigram:\")\n",
    "for i in range(10):\n",
    "    print(generate(1,uni))\n",
    "print(\"\\nBigram:\")\n",
    "for i in range(10):\n",
    "    print(generate(2,bi))\n",
    "print(\"\\nTrigram:\")\n",
    "for i in range(10):\n",
    "    print(generate(3,tri))\n",
    "print(\"\\nQuadgram:\")\n",
    "for i in range(10):\n",
    "    print(generate(4,quad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP of  <s> i can t tell you yet </s>\n",
      "1  Gram  117.55\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> and by the way we re going to be saying merry christmas again </s>\n",
      "1  Gram  215.71\n",
      "2  Gram  9.05\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> i ll tell you what we re going to do right </s>\n",
      "1  Gram  93.57\n",
      "2  Gram  9.12\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> so much money </s>\n",
      "1  Gram  94.1\n",
      "2  Gram  11.31\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> i said if you attack iraq and you wipe it out iran is going to take over the entire middle east because you re going to ruin the balance </s>\n",
      "1  Gram  374.64\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> again many americans must wonder why we our politicians seem more interested in defending the borders of foreign countries than in defending their own </s>\n",
      "1  Gram  inf\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> our theme is very simple </s>\n",
      "1  Gram  289.64\n",
      "2  Gram  30.43\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> i m just doing what s right </s>\n",
      "1  Gram  112.36\n",
      "2  Gram  26.9\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> and by the way we have no choice </s>\n",
      "1  Gram  122.08\n",
      "2  Gram  12.12\n",
      "3  Gram  5.69\n",
      "4  Gram  inf\n",
      "PP of  <s> we re going to get jobs </s>\n",
      "1  Gram  76.81\n",
      "2  Gram  6.83\n",
      "3  Gram  4.98\n",
      "4  Gram  4.56\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "rand=[randrange(0,len(output_sent),1) for j in range(10)]\n",
    "def perplex(models):\n",
    "    for i in rand:\n",
    "        print(\"PP of \",' '.join(output_sent[i]))\n",
    "        for j in range(len(models)):\n",
    "            lm=models[j]\n",
    "            try:\n",
    "                print(j+1,\" Gram \",round(lm.perplexity(ngrams(output_sent[i],j+1)),2))\n",
    "            except:\n",
    "                print(j+1,\" Gram \",\"inf\")\n",
    "perplex([uni,bi,tri,quad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters:  5857\n",
      "Number of extracted sequences: 166838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((166838, 10), (166838, 5857))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = sorted(total_vocab)\n",
    "n_to_char = {n:char for n, char in enumerate(characters)}\n",
    "char_to_n = {char:n for n, char in enumerate(characters)}\n",
    "vocab_size = len(characters)\n",
    "print('Number of unique characters: ', vocab_size)\n",
    "X = []   # extracted sequences\n",
    "Y = []   # the target - the follow up character\n",
    "length = len(token)\n",
    "seq_length = 10   #number of characters to consider before predicting the following character\n",
    "for i in range(0, length - seq_length, 1):\n",
    "    sequence = token[i:i + seq_length]\n",
    "    label = token[i + seq_length]\n",
    "    X.append([char_to_n[char] for char in sequence])\n",
    "    Y.append(char_to_n[label])\n",
    "# print(X[100])\n",
    "print('Number of extracted sequences:', len(X))\n",
    "X_modified = np.reshape(X, (len(X), seq_length))\n",
    "# X_modified = X_modified / float(len(characters))\n",
    "Y_modified = np_utils.to_categorical(Y)\n",
    "X_modified.shape, Y_modified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 100)           585700    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5857)              1177257   \n",
      "=================================================================\n",
      "Total params: 2,003,757\n",
      "Trainable params: 2,003,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "166838/166838 [==============================] - 95s 567us/step - loss: 5.4042\n",
      "\n",
      "Epoch 00001: loss improved from inf to 5.40416, saving model to baseline-improvement-01-5.4042.hdf5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 100)           585700    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5857)              1177257   \n",
      "=================================================================\n",
      "Total params: 1,823,157\n",
      "Trainable params: 1,823,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "166838/166838 [==============================] - 68s 405us/step - loss: 5.3270\n",
      "\n",
      "Epoch 00001: loss improved from inf to 5.32699, saving model to vanila-improvement-01-5.3270.hdf5\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=10))\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    filepath=\"baseline-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(X_modified, Y_modified, epochs=1, batch_size=256,callbacks=callbacks_list)\n",
    "    return model\n",
    "def Vanila_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=10))\n",
    "    model.add(SimpleRNN(200))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    filepath=\"vanila-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(X_modified, Y_modified, epochs=1, batch_size=256,callbacks=callbacks_list)\n",
    "    return model\n",
    "lstm=LSTM_model()\n",
    "vanila=Vanila_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "see </s> <s> now he s getting out of the\n",
      "Output:\n",
      "see </s> <s> now he s getting out of the going to have </s> \n",
      "Input:\n",
      "controlled by you </s> <s> we re not going to\n",
      "Output:\n",
      "controlled by you </s> <s> we re not going to be a lot </s> \n"
     ]
    }
   ],
   "source": [
    "def generate(model,string_mapped=None):\n",
    "    if(string_mapped==None):\n",
    "        start = np.random.randint(0, len(X)-1) # or generate random start\n",
    "        string_mapped = list(X[start])\n",
    "    full_string = [n_to_char[value] for value in string_mapped]\n",
    "# print(start,full_string)\n",
    "    print(\"Input:\")\n",
    "    print(' '.join(full_string))\n",
    "    for i in range(30):\n",
    "        x = np.reshape(string_mapped,(1,len(string_mapped)))\n",
    "#         x = x / float(len(characters))\n",
    "    #     print(x)\n",
    "        pred_index = np.argmax(model.predict(x, verbose=0))\n",
    "        seq = [n_to_char[value] for value in string_mapped]\n",
    "    #     print(n_to_char[pred_index],pred_index)\n",
    "        full_string.append(n_to_char[pred_index])\n",
    "        string_mapped.append(pred_index)  # add the predicted character to the end\n",
    "        string_mapped = string_mapped[1:] # shift the string one character forward by removing pos. 0\n",
    "        if(n_to_char[pred_index]==\"</s>\"):\n",
    "            break\n",
    "    txt=\"\"\n",
    "    for char in full_string:\n",
    "        txt = txt+char+\" \"\n",
    "#     print(start)\n",
    "    print(\"Output:\")\n",
    "    print(txt)\n",
    "generate(lstm)\n",
    "generate(vanila)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have shape (5857,) but got array with shape (5689,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-3c6a52d2715d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Perplexity \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-151-3c6a52d2715d>\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     print(y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have shape (5857,) but got array with shape (5689,)"
     ]
    }
   ],
   "source": [
    "output_sequences = []\n",
    "for i in range(1,10):\n",
    "    n_gram_sequence = token2[:i+1]\n",
    "    output_sequences.append(n_gram_sequence)\n",
    "x_test, y_tes = output_sequences[:][:-1],output_sequences[:][-1]\n",
    "x_test = np.array(pad_sequences(x_test,maxlen=10, padding='pre',dtype='U25',value='0'))\n",
    "y_test = []\n",
    "for i in range(len(y_tes)):\n",
    "            y_test.append(char_to_n[y_tes[i]])\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "x_test= np.reshape(x_test,(len(x_test),10))\n",
    "def perplexity(model):\n",
    "#     print(y_test)\n",
    "    score = model.evaluate(x_test, y_test, verbose=False)\n",
    "    print('Test score: ', score[0])    \n",
    "    print('Test accuracy: ', score[1])\n",
    "    print(\"Perplexity \", np.exp(score[0]))\n",
    "perplexity(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense,Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "# import numpy as np\n",
    "# , label = input_ngram[:][:-1],input_ngram[:][-1]\n",
    "label=[ele[-1] for ele in input_ngram]\n",
    "predictors=[list(ele[:-1]) for ele in input_ngram]\n",
    "print(label[:10])\n",
    "label = encoder.fit_transform(label)\n",
    "print(label[:10])\n",
    "def create_model(predictors, label, max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model.fit(predictors, label, epochs=100, verbose=1)\n",
    "\n",
    "def generate_text(seed_text, next_words, max_sequence_len, model):\n",
    "    for j in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen= \n",
    "                             max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "  \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "model = create_model(predictors,label,3,len(vocab))\n",
    "text = generate_text(\"we have\",1,2, model)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import numpy as np \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def dataset_preparation(data):\n",
    "\n",
    "\t# basic cleanup\n",
    "\tcorpus = data.lower().split(\"\\n\")\n",
    "\n",
    "\t# tokenization\t\n",
    "\ttokenizer.fit_on_texts(corpus)\n",
    "\ttotal_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "\t# create input sequences using list of tokens\n",
    "\tinput_sequences = []\n",
    "\tfor line in corpus:\n",
    "\t\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\t\tfor i in range(1, len(token_list)):\n",
    "\t\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "\t# pad sequences \n",
    "\tmax_sequence_len = max([len(x) for x in input_sequences])\n",
    "\tinput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "\t# create predictors and label\n",
    "\tpredictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\tlabel = ku.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "\treturn predictors, label, max_sequence_len, total_words\n",
    "\n",
    "def create_model(predictors, label, max_sequence_len, total_words):\n",
    "\t\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "\tmodel.add(LSTM(150, return_sequences = True))\n",
    "\t# model.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(100))\n",
    "\tmodel.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\tearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\tmodel.fit(predictors, label, epochs=10, verbose=1, callbacks=[earlystop])\n",
    "\tprint(model.summary())\n",
    "\treturn model \n",
    "\n",
    "def generate_text(seed_text, next_words, max_sequence_len):\n",
    "\tfor _ in range(next_words):\n",
    "\t\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\t\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\t\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\t\t\n",
    "\t\toutput_word = \"\"\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == predicted:\n",
    "\t\t\t\toutput_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\tseed_text += \" \" + output_word\n",
    "\treturn seed_text\n",
    "\n",
    "\n",
    "\n",
    "data = open('speeches.txt').read()\n",
    "\n",
    "predictors, label, max_sequence_len, total_words = dataset_preparation(data)\n",
    "model = create_model(predictors, label, max_sequence_len, total_words)\n",
    "print(generate_text(\"we have\", 3, max_sequence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

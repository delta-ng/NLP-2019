{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processing & Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import ngrams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import re\n",
    "f = open(\"speeches.txt\", \"r\")\n",
    "donald = f.read()\n",
    "donald=donald.lower()\n",
    "sent_tokenize_list = sent_tokenize(donald)\n",
    "# print(sent_tokenize_list[:10])\n",
    "for i in range(len(sent_tokenize_list)):\n",
    "#     sent_tokenize_list[i]=re.sub(r'\\n', '',sent_tokenize_list[i])\n",
    "    sent_tokenize_list[i]=re.sub(r'[^\\w]', ' ',sent_tokenize_list[i])\n",
    "X_train, X_test = train_test_split(sent_tokenize_list, test_size=0.2 , random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating vocab and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mainstream\n"
     ]
    }
   ],
   "source": [
    "vocab=[]\n",
    "token=[]\n",
    "for i in range(len(X_train)):\n",
    "    temp=word_tokenize(X_train[i])\n",
    "    token.append('<s>')\n",
    "    token.extend(temp)\n",
    "    token.append('</s>')\n",
    "#     vocab=np.concatenate((vocab,np.array(word_tokenize(X_train[i]))), axis=None)\n",
    "vocab=np.unique(token)\n",
    "# print(len(vocab))\n",
    "print(vocab[3000])\n",
    "output_sent=[]\n",
    "for i in range(len(X_test)):\n",
    "    temp=word_tokenize(X_test[i])\n",
    "    token1=[]\n",
    "    token1.append('<s>')\n",
    "    token1.extend(temp)\n",
    "    token1.append('</s>')\n",
    "    output_sent.append(token1)\n",
    "# vocab=np.unique(np.concatenate((vocab,np.array(token1)), axis=None))\n",
    "# print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of  1 -gram Theoretically possible:  5406\n",
      "No of  1 -gram Observed:  5406\n",
      "No of  2 -gram Theoretically possible:  29224836\n",
      "No of  2 -gram Observed:  41930\n",
      "No of  3 -gram Theoretically possible:  157989463416\n",
      "No of  3 -gram Observed:  84828\n",
      "No of  4 -gram Theoretically possible:  854091039226896\n",
      "No of  4 -gram Observed:  118036\n"
     ]
    }
   ],
   "source": [
    "def ngram(n):\n",
    "    input_ngram = list(everygrams(token,max_len=n))\n",
    "    lm = MLE(n)\n",
    "    lm.fit([input_ngram], vocabulary_text=vocab)\n",
    "    return lm\n",
    "for i in range(1,5):\n",
    "    print(\"No of \",i,\"-gram Theoretically possible: \",len(vocab)**i)\n",
    "    print(\"No of \",i,\"-gram Observed: \",len(set(ngrams(token,i))))\n",
    "uni=ngram(1)\n",
    "bi=ngram(2)\n",
    "tri=ngram(3)\n",
    "quad=ngram(4)\n",
    "# print(\"lo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram:\n",
      "<s> political <s> in you this why to </s>\n",
      "<s> did and biggest when other will don he complex were income as love <s> new thing <s> the s which <s> much was don well pages shouldn the and all it </s>\n",
      "<s> are too product </s>\n",
      "<s> <s> the they <s> chance they to go as </s>\n",
      "<s> blew to that have to better hispanics i from comparing they great all 23 what a be <s> big a touch else people employed from i of honored the </s>\n",
      "<s> </s>\n",
      "<s> back yes who energy who the i if actually <s> off get me go of nomination <s> stronger it of that it hillary they was to believe and right is result run is this said they know of <s> in going protect with think did </s>\n",
      "<s> <s> see i i who we an when </s>\n",
      "<s> t so we from </s>\n",
      "<s> </s>\n",
      "\n",
      "Bigram:\n",
      "<s> so thank you the bomb by the things that she s being a phenomenal relationships with an example a web site </s>\n",
      "<s> trump </s>\n",
      "<s> it is not let him something that i went around the heads most qualified you know if i can convince them in those has a sitting there is going to fix it it s a fight </s>\n",
      "<s> don t see local doctors </s>\n",
      "<s> morning and a christian are democrats no other words i mean yesterday for so much he s not been phenomenal </s>\n",
      "<s> and they d say well we re all over a trader </s>\n",
      "<s> so great people that border </s>\n",
      "<s> i have that s what it </s>\n",
      "<s> it draws flies </s>\n",
      "<s> you know the area as you even wanted her email problem </s>\n",
      "\n",
      "Trigram:\n",
      "<s> thank you you away re then country we to you me they not </s>\n",
      "<s> there will any japan he <s> now the our it has right get in and 50 i you example the 15 our great doesn but really anyways mention essentially in and t to in times m <s> <s> of sense i 16 to has </s>\n",
      "<s> an embarrassment answer the <s> out gene want cross the straightened we world have because i <s> yeah thing loss and south believe <s> highly very <s> a </s>\n",
      "<s> with that mexico him <s> through 000 had really bridges </s>\n",
      "<s> we need re <s> <s> it on here he state know the 3 look states whole that in said said by establishment to my five clinton sense fear so i we t </s>\n",
      "<s> you see know he all america fail aren to i they i the said </s>\n",
      "<s> i wonder m lot if the but families to </s>\n",
      "<s> by the 13 <s> look will you he to they my a drop to inclined fox looks for worse lot re started do will <s> me it i how <s> happened far i all a get year dishonest what of something to know stand <s> nobody stop giving actually lot it i campaign you t great loaded going calls and groups minute kill was <s> here up the the of 5 know in a <s> unemployment argue just say deal the i owe couple page are going to being in like want win ridiculous country the by or to and i to the one do the you battlefield a </s>\n",
      "<s> i don ll a nobody d said cameras want <s> to a <s> our vote <s> now the s do even of all we it and </s>\n",
      "<s> they spend don even <s> t s american have today temperament border millions say and are s want we history and related </s>\n",
      "\n",
      "Quadgram:\n",
      "<s> i don t t t goes greatest </s>\n",
      "<s> the worst in regulations </s>\n",
      "<s> we re going going not s many i and obamacare negotiate hedge <s> budget <s> and even i 100 either bad was </s>\n",
      "<s> a couple of of of is it going over he to think going the the no to woman <s> you supposed giving </s>\n",
      "<s> that commitment is is </s>\n",
      "<s> our country could massive doesn relationship he see <s> mean <s> side <s> look and </s>\n",
      "<s> we re going going young they keep don a </s>\n",
      "<s> doesn t win like think you re <s> age don already what people here incredible and were on credit oklahoma dollars a do politicians all than importantly struggle s but <s> her </s>\n",
      "<s> it s not supply a t i world paper them nothing </s>\n",
      "<s> and honestly he it much <s> all going bad very nabsico <s> ve we ll of the cleaned know do in incredible in one and woman recently just closing photographic about </s>\n"
     ]
    }
   ],
   "source": [
    "def generate(n,model):\n",
    "    start=\"<s>\"\n",
    "    sentence=[start]\n",
    "    if(n==1):\n",
    "        while(start!=\"</s>\"):\n",
    "            index=np.argmax(np.random.multinomial(1,[model.score(i) for i in vocab],size = None))\n",
    "            sentence.append(vocab[index])\n",
    "            start=vocab[index]\n",
    "    elif(n==2):\n",
    "        while(start!=\"</s>\"):\n",
    "            index=np.argmax(np.random.multinomial(1,[model.score(i,[start]) for i in vocab],size = None))\n",
    "            sentence.append(vocab[index])\n",
    "            start=vocab[index]\n",
    "    elif(n==3):\n",
    "        start_arr=[start]\n",
    "        index=np.argmax(np.random.multinomial(1,[model.score(i,[start]) for i in vocab],size = None))\n",
    "        sentence.append(vocab[index])\n",
    "        start_arr.append(vocab[index])\n",
    "        while(start!=\"</s>\"):\n",
    "            index=np.argmax(np.random.multinomial(1,[model.score(i,start_arr) for i in vocab],size = None))\n",
    "            sentence.append(vocab[index])\n",
    "            start=vocab[index]\n",
    "            start_arr=start_arr[1:]\n",
    "    elif(n==4):\n",
    "        start_arr=[start]\n",
    "        index=np.argmax(np.random.multinomial(1,[model.score(i,[start]) for i in vocab],size = None))\n",
    "        sentence.append(vocab[index])\n",
    "        start_arr.append(vocab[index])\n",
    "        index=np.argmax(np.random.multinomial(1,[model.score(i,start_arr) for i in vocab],size = None))\n",
    "        sentence.append(vocab[index])\n",
    "        start_arr.append(vocab[index])\n",
    "        while(start!=\"</s>\"):\n",
    "            index=np.argmax(np.random.multinomial(1,[model.score(i,start_arr) for i in vocab],size = None))\n",
    "            sentence.append(vocab[index])\n",
    "            start=vocab[index]\n",
    "            start_arr=start_arr[1:]\n",
    "    return ' '.join(sentence)\n",
    "print(\"Unigram:\")\n",
    "for i in range(10):\n",
    "    print(generate(1,uni))\n",
    "print(\"\\nBigram:\")\n",
    "for i in range(10):\n",
    "    print(generate(2,bi))\n",
    "print(\"\\nTrigram:\")\n",
    "for i in range(10):\n",
    "    print(generate(3,tri))\n",
    "print(\"\\nQuadgram:\")\n",
    "for i in range(10):\n",
    "    print(generate(4,quad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP of  <s> i m in for a lot of money </s>\n",
      "1  Gram  89.53\n",
      "2  Gram  12.42\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> how does this kind of immigration make our life better </s>\n",
      "1  Gram  395.38\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> not assets </s>\n",
      "1  Gram  153.64\n",
      "2  Gram  101.29\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> and soon the country s going to start winning winning winning </s>\n",
      "1  Gram  255.38\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> and hillary should have approved it </s>\n",
      "1  Gram  179.16\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> number one hostages before we utter the first word </s>\n",
      "1  Gram  703.0\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> believe me </s>\n",
      "1  Gram  80.75\n",
      "2  Gram  15.92\n",
      "3  Gram  1.52\n",
      "4  Gram  1.88\n",
      "PP of  <s> i m going on vacation </s>\n",
      "1  Gram  156.28\n",
      "2  Gram  13.05\n",
      "3  Gram  13.89\n",
      "4  Gram  6.02\n",
      "PP of  <s> he knew about it </s>\n",
      "1  Gram  111.38\n",
      "2  Gram  18.37\n",
      "3  Gram  3.6\n",
      "4  Gram  1.19\n",
      "PP of  <s> that s why being a builder and a great builder and a very successful builder i think will greatly help </s>\n",
      "1  Gram  496.94\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "rand=[randrange(0,len(output_sent),1) for j in range(10)]\n",
    "def perplex(models):\n",
    "    for i in rand:\n",
    "        print(\"PP of \",' '.join(output_sent[i]))\n",
    "        for j in range(len(models)):\n",
    "            lm=models[j]\n",
    "            try:\n",
    "                print(j+1,\" Gram \",round(lm.perplexity(ngrams(output_sent[i],j+1)),2))\n",
    "            except:\n",
    "                print(j+1,\" Gram \",\"inf\")\n",
    "perplex([uni,bi,tri,quad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import ngrams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import everygrams\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"speeches.txt\", \"r\")\n",
    "donald = f.read()\n",
    "donald=donald.lower()\n",
    "sent_tokenize_list = sent_tokenize(donald)\n",
    "for i in range(len(sent_tokenize_list)):\n",
    "    sent_tokenize_list[i]=re.sub(r'[^\\w]', ' ',sent_tokenize_list[i])\n",
    "X_train, X_test = train_test_split(sent_tokenize_list, test_size=0.2 , random_state=42)\n",
    "vocab=[]\n",
    "token=[]\n",
    "for i in range(len(X_train)):\n",
    "    temp=word_tokenize(X_train[i])\n",
    "    token.append('<s>')\n",
    "    token.extend(temp)\n",
    "    token.append('</s>')\n",
    "vocab=np.unique(token)\n",
    "# print(vocab[3000])\n",
    "output_sent=[]\n",
    "token2=[]\n",
    "for i in range(len(X_test)):\n",
    "    temp=word_tokenize(X_test[i])\n",
    "    token1=[]\n",
    "    token1.append('<s>')\n",
    "    token1.extend(temp)\n",
    "    token1.append('</s>')\n",
    "    output_sent.append(token1)\n",
    "    token2.extend(token1)\n",
    "total_vocab=np.unique(np.concatenate((token2,vocab),axis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of  1 - gram Theoretically possible:  5406\n",
      "No of  1 - gram Observed:  5406\n",
      "No of  2 - gram Theoretically possible:  29224836\n",
      "No of  2 - gram Observed:  41930\n",
      "No of  3 - gram Theoretically possible:  157989463416\n",
      "No of  3 - gram Observed:  84828\n",
      "No of  4 - gram Theoretically possible:  854091039226896\n",
      "No of  4 - gram Observed:  118036\n"
     ]
    }
   ],
   "source": [
    "def ngram(n):\n",
    "    input_ngram = list(everygrams(token,max_len=n))\n",
    "    lm = MLE(n)\n",
    "    lm.fit([input_ngram], vocabulary_text=vocab)\n",
    "    return lm\n",
    "for i in range(1,5):\n",
    "    print(\"No of \",i,\"- gram Theoretically possible: \",len(vocab)**i)\n",
    "    print(\"No of \",i,\"- gram Observed: \",len(set(ngrams(token,i))))\n",
    "uni=ngram(1)\n",
    "bi=ngram(2)\n",
    "tri=ngram(3)\n",
    "quad=ngram(4)\n",
    "# print(\"lo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram:\n",
      "<s> is be this the i get be <s> strongly met presidential it <s> after </s>\n",
      "<s> tax s trump my those s into end you they </s>\n",
      "<s> way wrong i we but corey a he in <s> congratulations i started say your trillion and <s> <s> give future because that vigilant you they </s>\n",
      "<s> that us better chance up i to understand and <s> he recently <s> <s> we will with i hillary know make i such <s> i i losing election the re from thing got we ending getting very <s> you </s>\n",
      "<s> is do pulled will </s>\n",
      "<s> actually but you re believe watch trump is now false i she be if with below time trump teleprompter said has he report they a be they <s> </s>\n",
      "<s> you which <s> to china up say <s> speech yesterday leave the now florida so hillary said bernie replace community allowing little </s>\n",
      "<s> involved establishment be actually the </s>\n",
      "<s> going ll </s>\n",
      "<s> the were can and have there </s>\n",
      "\n",
      "Bigram:\n",
      "<s> and tough cookie </s>\n",
      "<s> he likes of them </s>\n",
      "<s> he s attacked </s>\n",
      "<s> okay thank you re going on top of them </s>\n",
      "<s> and begin by massive massive </s>\n",
      "<s> so cute </s>\n",
      "<s> done an endorsement </s>\n",
      "<s> an incredible spirits that they don t think the history of free and honestly it s really great businessman you know </s>\n",
      "<s> get unless you know we can t win anymore essentially nothing </s>\n",
      "<s> and she s we are going to start going to come to shake your country </s>\n",
      "\n",
      "Trigram:\n",
      "<s> they re can it much </s>\n",
      "<s> but i i disarm we short we i person i </s>\n",
      "<s> his advice group fortune sexism so going so had must know the the i </s>\n",
      "<s> i went want something in and political get don see but to country great </s>\n",
      "<s> which they i in early you come people american weeks i sad <s> that mexico people wall sent our ve military everyone of thing wanted <s> was <s> has they of them a </s>\n",
      "<s> and we all i but our these fight i tour own got trump to are i you everybody re hear but <s> her t them get </s>\n",
      "<s> it got s was told soon great built in and </s>\n",
      "<s> now i they they in it <s> you building <s> they as it a percent </s>\n",
      "<s> here we a know <s> built that oh so s </s>\n",
      "<s> you know don do one him but trade is east disaster because </s>\n",
      "\n",
      "Quadgram:\n",
      "<s> nothing </s> <s> <s> <s> she <s> s disaster or and sir want side bags <s> </s>\n",
      "<s> you know we iconic what told make <s> why </s>\n",
      "<s> i don t t t don if oh i make country the deals financial <s> <s> </s>\n",
      "<s> they said whoa what he are question also a like lots first devastating i understand </s>\n",
      "<s> they announced they they </s>\n",
      "<s> i didn t t t lose </s>\n",
      "<s> they have zero the illegal communities </s>\n",
      "<s> i just think want want but i no donaldjtrump i the </s>\n",
      "<s> i would say have somebody we point good unbelievable <s> it <s> </s>\n",
      "<s> and he s came s to our at is and going don benefit said friends <s> he that off 5 </s>\n"
     ]
    }
   ],
   "source": [
    "def generate(n,model):\n",
    "    start=\"<s>\"\n",
    "    sentence=[start]\n",
    "    if(n==1):\n",
    "        while(start!=\"</s>\"):\n",
    "            index=np.argmax(np.random.multinomial(1,[model.score(i) for i in vocab],size = None))\n",
    "            sentence.append(vocab[index])\n",
    "            start=vocab[index]\n",
    "    elif(n==2):\n",
    "        while(start!=\"</s>\"):\n",
    "            index=np.argmax(np.random.multinomial(1,[model.score(i,[start]) for i in vocab],size = None))\n",
    "            sentence.append(vocab[index])\n",
    "            start=vocab[index]\n",
    "    elif(n==3):\n",
    "        start_arr=[start]\n",
    "        index=np.argmax(np.random.multinomial(1,[model.score(i,[start]) for i in vocab],size = None))\n",
    "        sentence.append(vocab[index])\n",
    "        start_arr.append(vocab[index])\n",
    "        while(start!=\"</s>\"):\n",
    "            index=np.argmax(np.random.multinomial(1,[model.score(i,start_arr) for i in vocab],size = None))\n",
    "            sentence.append(vocab[index])\n",
    "            start=vocab[index]\n",
    "            start_arr=start_arr[1:]\n",
    "    elif(n==4):\n",
    "        start_arr=[start]\n",
    "        index=np.argmax(np.random.multinomial(1,[model.score(i,[start]) for i in vocab],size = None))\n",
    "        sentence.append(vocab[index])\n",
    "        start_arr.append(vocab[index])\n",
    "        index=np.argmax(np.random.multinomial(1,[model.score(i,start_arr) for i in vocab],size = None))\n",
    "        sentence.append(vocab[index])\n",
    "        start_arr.append(vocab[index])\n",
    "        while(start!=\"</s>\"):\n",
    "            index=np.argmax(np.random.multinomial(1,[model.score(i,start_arr) for i in vocab],size = None))\n",
    "            sentence.append(vocab[index])\n",
    "            start=vocab[index]\n",
    "            start_arr=start_arr[1:]\n",
    "    return ' '.join(sentence)\n",
    "print(\"Unigram:\")\n",
    "for i in range(10):\n",
    "    print(generate(1,uni))\n",
    "print(\"\\nBigram:\")\n",
    "for i in range(10):\n",
    "    print(generate(2,bi))\n",
    "print(\"\\nTrigram:\")\n",
    "for i in range(10):\n",
    "    print(generate(3,tri))\n",
    "print(\"\\nQuadgram:\")\n",
    "for i in range(10):\n",
    "    print(generate(4,quad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP of  <s> i wasn t looking for the credit </s>\n",
      "1  Gram  201.68\n",
      "2  Gram  28.31\n",
      "3  Gram  13.89\n",
      "4  Gram  1.51\n",
      "PP of  <s> you have to vet it </s>\n",
      "1  Gram  96.52\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> nothing s gon na get done </s>\n",
      "1  Gram  486.26\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> and i don t understand </s>\n",
      "1  Gram  72.06\n",
      "2  Gram  9.32\n",
      "3  Gram  8.64\n",
      "4  Gram  12.81\n",
      "PP of  <s> i mean it s a money making industry ok </s>\n",
      "1  Gram  226.65\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> i just want a job </s>\n",
      "1  Gram  94.17\n",
      "2  Gram  25.48\n",
      "3  Gram  12.48\n",
      "4  Gram  inf\n",
      "PP of  <s> so i just want to let you know it s very important </s>\n",
      "1  Gram  115.07\n",
      "2  Gram  15.54\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> he didn t win anything </s>\n",
      "1  Gram  161.15\n",
      "2  Gram  18.06\n",
      "3  Gram  8.64\n",
      "4  Gram  2.28\n",
      "PP of  <s> i ll bring back our jobs from china from mexico from japan from so many places </s>\n",
      "1  Gram  409.43\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n",
      "PP of  <s> altogether under the clinton plan you d be admitting hundreds of thousands of refugees from the middle east with no system to vet them or to prevent the radicalization of the children and their children </s>\n",
      "1  Gram  751.1\n",
      "2  Gram  inf\n",
      "3  Gram  inf\n",
      "4  Gram  inf\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "rand=[randrange(0,len(output_sent),1) for j in range(10)]\n",
    "def perplex(models):\n",
    "    for i in rand:\n",
    "        print(\"PP of \",' '.join(output_sent[i]))\n",
    "        for j in range(len(models)):\n",
    "            lm=models[j]\n",
    "            try:\n",
    "                print(j+1,\" Gram \",round(lm.perplexity(ngrams(output_sent[i],j+1)),2))\n",
    "            except:\n",
    "                print(j+1,\" Gram \",\"inf\")\n",
    "perplex([uni,bi,tri,quad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability : Some of the sentence are gramatically correct. But, there are a lot of flaw bcs the test data has new words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters:  5857\n",
      "Number of extracted sequences: 166838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((166838, 10), (166838, 5857))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = sorted(total_vocab)\n",
    "n_to_char = {n:char for n, char in enumerate(characters)}\n",
    "char_to_n = {char:n for n, char in enumerate(characters)}\n",
    "vocab_size = len(characters)\n",
    "print('Number of unique characters: ', vocab_size)\n",
    "X = []   # extracted sequences\n",
    "Y = []   # the target - the follow up character\n",
    "length = len(token)\n",
    "seq_length = 10   #number of characters to consider before predicting the following character\n",
    "for i in range(0, length - seq_length, 1):\n",
    "    sequence = token[i:i + seq_length]\n",
    "    label = token[i + seq_length]\n",
    "    X.append([char_to_n[char] for char in sequence])\n",
    "    Y.append(char_to_n[label])\n",
    "# print(X[100])\n",
    "print('Number of extracted sequences:', len(X))\n",
    "X_modified = np.reshape(X, (len(X), seq_length))\n",
    "# X_modified = X_modified / float(len(characters))\n",
    "Y_modified = np_utils.to_categorical(Y)\n",
    "X_modified.shape, Y_modified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 10, 100)           585700    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5857)              1177257   \n",
      "=================================================================\n",
      "Total params: 2,003,757\n",
      "Trainable params: 2,003,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "166838/166838 [==============================] - 99s 594us/step - loss: 5.6656\n",
      "\n",
      "Epoch 00001: loss improved from inf to 5.66563, saving model to baseline-improvement-01-5.6656.hdf5\n",
      "Epoch 2/5\n",
      "166838/166838 [==============================] - 98s 587us/step - loss: 4.8678\n",
      "\n",
      "Epoch 00002: loss improved from 5.66563 to 4.86780, saving model to baseline-improvement-02-4.8678.hdf5\n",
      "Epoch 3/5\n",
      "166838/166838 [==============================] - 94s 566us/step - loss: 4.3749\n",
      "\n",
      "Epoch 00003: loss improved from 4.86780 to 4.37488, saving model to baseline-improvement-03-4.3749.hdf5\n",
      "Epoch 4/5\n",
      "166838/166838 [==============================] - 93s 557us/step - loss: 4.1668\n",
      "\n",
      "Epoch 00004: loss improved from 4.37488 to 4.16679, saving model to baseline-improvement-04-4.1668.hdf5\n",
      "Epoch 5/5\n",
      "166838/166838 [==============================] - 97s 580us/step - loss: 4.0248\n",
      "\n",
      "Epoch 00005: loss improved from 4.16679 to 4.02478, saving model to baseline-improvement-05-4.0248.hdf5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 10, 100)           585700    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5857)              1177257   \n",
      "=================================================================\n",
      "Total params: 1,823,157\n",
      "Trainable params: 1,823,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "166838/166838 [==============================] - 60s 361us/step - loss: 5.3644\n",
      "\n",
      "Epoch 00001: loss improved from inf to 5.36439, saving model to vanila-improvement-01-5.3644.hdf5\n",
      "Epoch 2/5\n",
      "166838/166838 [==============================] - 58s 350us/step - loss: 4.3732\n",
      "\n",
      "Epoch 00002: loss improved from 5.36439 to 4.37324, saving model to vanila-improvement-02-4.3732.hdf5\n",
      "Epoch 3/5\n",
      "166838/166838 [==============================] - 61s 366us/step - loss: 4.0471\n",
      "\n",
      "Epoch 00003: loss improved from 4.37324 to 4.04710, saving model to vanila-improvement-03-4.0471.hdf5\n",
      "Epoch 4/5\n",
      "166838/166838 [==============================] - 62s 371us/step - loss: 3.8295\n",
      "\n",
      "Epoch 00004: loss improved from 4.04710 to 3.82951, saving model to vanila-improvement-04-3.8295.hdf5\n",
      "Epoch 5/5\n",
      "166838/166838 [==============================] - 60s 357us/step - loss: 3.6553\n",
      "\n",
      "Epoch 00005: loss improved from 3.82951 to 3.65527, saving model to vanila-improvement-05-3.6553.hdf5\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=10))\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    filepath=\"lstm.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(X_modified, Y_modified, epochs=5, batch_size=256,callbacks=callbacks_list)\n",
    "    return model\n",
    "def Vanila_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=10))\n",
    "    model.add(SimpleRNN(200))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    filepath=\"vanila.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(X_modified, Y_modified, epochs=5, batch_size=256,callbacks=callbacks_list)\n",
    "    return model\n",
    "lstm=LSTM_model()\n",
    "vanila=Vanila_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n",
      "Input:\n",
      "s a scam </s> <s> so the wounded warriors all\n",
      "Output:\n",
      "s a scam </s> <s> so the wounded warriors all of the way i m not going to be a lot of the world </s> \n",
      "Input:\n",
      "was like the academy awards </s> <s> i mean when\n",
      "Output:\n",
      "was like the academy awards </s> <s> i mean when you know i m going to be a lot of the world </s> \n",
      "Input:\n",
      "be the stupid people anymore </s> <s> some are </s>\n",
      "Output:\n",
      "be the stupid people anymore </s> <s> some are </s> <s> i m going to be a lot of the world </s> \n",
      "Input:\n",
      "</s> <s> it was also hillary clinton as secretary of\n",
      "Output:\n",
      "</s> <s> it was also hillary clinton as secretary of the world </s> \n",
      "Input:\n",
      "from people i ll negotiate </s> <s> but we have\n",
      "Output:\n",
      "from people i ll negotiate </s> <s> but we have to be a lot of the world </s> \n",
      "\n",
      "Vanilla\n",
      "Input:\n",
      "fest </s> <s> but just in a nutshell i m\n",
      "Output:\n",
      "fest </s> <s> but just in a nutshell i m going to do it </s> \n",
      "Input:\n",
      "years old a child a beautiful child went to have\n",
      "Output:\n",
      "years old a child a beautiful child went to have a lot of money </s> \n",
      "Input:\n",
      "certain general i m not going to insult him </s>\n",
      "Output:\n",
      "certain general i m not going to insult him </s> <s> i don t know </s> \n",
      "Input:\n",
      "and everything else that to me was probably the single\n",
      "Output:\n",
      "and everything else that to me was probably the single day i m going to do it </s> \n",
      "Input:\n",
      "another thing </s> <s> but we have to do something\n",
      "Output:\n",
      "another thing </s> <s> but we have to do something </s> \n"
     ]
    }
   ],
   "source": [
    "def generate(model,string_mapped=None):\n",
    "    if(string_mapped==None):\n",
    "        start = np.random.randint(0, len(X)-1) # or generate random start\n",
    "        string_mapped = list(X[start])\n",
    "    full_string = [n_to_char[value] for value in string_mapped]\n",
    "# print(start,full_string)\n",
    "    print(\"Input:\")\n",
    "    print(' '.join(full_string))\n",
    "    for i in range(30):\n",
    "        x = np.reshape(string_mapped,(1,len(string_mapped)))\n",
    "#         x = x / float(len(characters))\n",
    "    #     print(x)\n",
    "        pred_index = np.argmax(model.predict(x, verbose=0))\n",
    "        seq = [n_to_char[value] for value in string_mapped]\n",
    "    #     print(n_to_char[pred_index],pred_index)\n",
    "        full_string.append(n_to_char[pred_index])\n",
    "        string_mapped.append(pred_index)  # add the predicted character to the end\n",
    "        string_mapped = string_mapped[1:] # shift the string one character forward by removing pos. 0\n",
    "        if(n_to_char[pred_index]==\"</s>\"):\n",
    "            break\n",
    "    txt=\"\"\n",
    "    for char in full_string:\n",
    "        txt = txt+char+\" \"\n",
    "#     print(start)\n",
    "    print(\"Output:\")\n",
    "    print(txt)\n",
    "print(\"LSTM\")\n",
    "generate(lstm)\n",
    "generate(lstm)\n",
    "generate(lstm)\n",
    "generate(lstm)\n",
    "generate(lstm)\n",
    "print(\"\\nVanilla\")\n",
    "generate(vanila)\n",
    "generate(vanila)\n",
    "generate(vanila)\n",
    "generate(vanila)\n",
    "generate(vanila)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n",
      "Test score:  4.187699517849314\n",
      "Perplexity  65.87108127788211\n",
      "Vanilla\n",
      "Test score:  3.9830074119721965\n",
      "Perplexity  53.67822425786633\n"
     ]
    }
   ],
   "source": [
    "length = len(token2)\n",
    "X=[]\n",
    "Y=[]\n",
    "seq_length = 10   #number of characters to consider before predicting the following character\n",
    "for i in range(0, length - seq_length, 1):\n",
    "    sequence = token2[i:i + seq_length]\n",
    "    label = token2[i + seq_length]\n",
    "    X.append([char_to_n[char] for char in sequence])\n",
    "    Y.append(char_to_n[label])\n",
    "# print(X[100])\n",
    "# print('Number of extracted sequences:', len(X))\n",
    "X_test = np.reshape(X, (len(X), seq_length))\n",
    "# X_modified = X_modified / float(len(characters))\n",
    "Y_test = np_utils.to_categorical(Y)\n",
    "def Perplex(model):\n",
    "    score = model.evaluate(X_test, Y_test,verbose=0)\n",
    "    print('Test score: ', score)    \n",
    "    # print('Test accuracy: ', score[1])\n",
    "    print(\"Perplexity \", np.exp(score))\n",
    "print(\"LSTM\")\n",
    "Perplex(lstm)\n",
    "print(\"Vanilla\")\n",
    "Perplex(vanila)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability : Does a wonderful job in producing new sentence. But, there are a lot of grammatical mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does Neural Network better than Classical Approach, if so why? If not why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity: RNN works better than LSTM and unigram. When it comes to train data, n-gram model works well but the problem occur when we evaluate it on test data. Since it hasn't seen the test data the probablity of most of the grams are zero when with smoothing the problem can be solved only till a limit. With neural net this problem is solved to a great extend. As the model learns from the train data. It searches for similarities not for exact match."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
